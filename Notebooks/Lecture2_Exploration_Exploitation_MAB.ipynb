{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lecture2-Exploration/Exploitation-MAB.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPdxT6DleIdaWjXeRBbqPNx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/llealgt/RL_Workshop_Guatemala/blob/master/Notebooks/Lecture2_Exploration_Exploitation_MAB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4b8x3OvKD8v",
        "colab_type": "text"
      },
      "source": [
        "# Exploración y Explotación en Multi-Armed Bandits\n",
        "\n",
        "## Material y Background\n",
        "Basado no solo en el curso de David Silver, también en material de Hado Van Hasselt y el libro de Sutton y Barto\n",
        "\n",
        "\n",
        "\n",
        "*   https://hadovanhasselt.files.wordpress.com/2016/01/xx2.pdf\n",
        "*   http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/XX.pdf\n",
        "*   Sutton y Barto capítulo 2\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqzgY3yuKKSb",
        "colab_type": "text"
      },
      "source": [
        "## Recapitulando\n",
        "\n",
        "*   Reinforcement learning es la ciencia de \"aprender a tomar desicines\"\n",
        "*   Un agente puede aprender una o más de las siguientes opciones:\n",
        "  *   Política\n",
        "  *   Función de evaluación\n",
        "  *   Modelo\n",
        "*   El problema general involucra tomar en cuenta el tiempo y consecuencias a futuro\n",
        "*   Las desiciones afectan la recompenza, el conocimiento interno(estado del agente) y el estado del ambiente\n",
        "\n",
        "![RL_cycle](https://www.unite.ai/wp-content/uploads/2019/10/Rl_agent.png)\n",
        "\n",
        "\n",
        "*Fuente:* [Daniel Nelson](https://www.unite.ai/what-is-reinforcement-learning/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNh03G_0LTHy",
        "colab_type": "text"
      },
      "source": [
        "## En Esta Sesión\n",
        "Versión simplificada del problema de RL\n",
        "\n",
        "\n",
        "*   Múltiples acciones, pero solo un estado.\n",
        "*   Las decisiones no afectan el estado del ambiente. (acciones pasadas no afectan el futuro)\n",
        "*   Formalmente: la distribución de la variable $R_{t}$ dado $A_{t}$ no depende del tiempo.\n",
        "*   Objetivo : optimizar la recompensa inmediata en un juego repetitivo.\n",
        "*   La historia no incluye observaciones (secuencia de acciones y recompensas)\n",
        "> $H_{t}=A_{1},R_{1},A_{2},R_{2},...,A_{t},R_{t}$ \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZHRb0xcMjPv",
        "colab_type": "text"
      },
      "source": [
        "## Ejemplo de la Rata \n",
        "\n",
        "![rata_ejemplo1](https://github.com/llealgt/RL_Workshop_Guatemala/blob/master/Assets/lect2_rat1_sample.png?raw=true)\n",
        "\n",
        "\n",
        "*Fuente:* [**UCL**](https://hadovanhasselt.files.wordpress.com/2016/01/xx2.pdf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VLf5RX1fM8ap"
      },
      "source": [
        "## Ejemplo de la Rata \n",
        "\n",
        "![rata_ejemplo1](https://github.com/llealgt/RL_Workshop_Guatemala/blob/master/Assets/lect2_rat2_sample.png?raw=true)\n",
        "\n",
        "\n",
        "*Fuente:* [UCL](https://hadovanhasselt.files.wordpress.com/2016/01/xx2.pdf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79_fO9YiMzLv",
        "colab_type": "text"
      },
      "source": [
        "## El Dilema de Explorar vs Explotar\n",
        "\n",
        "\n",
        "\n",
        "*   La toma de desiciones involucra una elección fundamental:\n",
        "  *   **Explotar** : tomar la mejor desición dada la información disponible actualmente.\n",
        "  *   **Explorar** : obtener mas información/incrementar el conocimiento actual\n",
        "*   La mejor estrategia a largo plazo puede requerir sacrificios a corto plazo.\n",
        "*   Se necesita ambas : obtener mas información para tomar las mejores desiciones\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af9zRUZ9N5TS",
        "colab_type": "text"
      },
      "source": [
        "## Ejemplos\n",
        "\n",
        "TODO: \n",
        "\n",
        "\n",
        "*   agregar ejemplos que ya están en sesión 1\n",
        "*   Buscar ejemplos interactivos o animaciones (o \"amarrarlo\" con los ejemplos interactivos de sesión 1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6eRVdJSP5yA",
        "colab_type": "text"
      },
      "source": [
        "## Distintos Enfoques/Principios para Exploración\n",
        "\n",
        "\n",
        "\n",
        "*   Exploración aleatoria\n",
        "  *   Explorar acciones aleatorias (ε-greedy)\n",
        "  *   Con probabilidad ε evaluar una acción al azar.\n",
        "*   Inicialización optimista\n",
        "  *   Asumir lo mejor hasta que se demuestre lo contrario.\n",
        "*   Optimismo ante la incerteza\n",
        "  *   Preferir acciones con valores inciertos.\n",
        "*   Probabilística\n",
        "  *   Seleccionar acciones de acuerdo a la probabilidad de ser la mejor.\n",
        "*   Estado de la información\n",
        "  *   Considerar la información del agente parte del estado.\n",
        "  *   Vista a futuro(lookahead) para ver como la información  ayuda con la recompensa.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vq6rc-qyTqIi",
        "colab_type": "text"
      },
      "source": [
        "## Multi-Armed Bandits\n",
        "Simplificación del problema de RL.\n",
        "\n",
        "\n",
        "\n",
        "*   Un MAB es una tupla (A,R)\n",
        "*   A es un conjunto conocido de m acciones (o \"brazos\")\n",
        "*   R es una distribución de probabilidad desconocida sobre las recompensas.\n",
        "*   En cada momento t el agente selecciona una accion a \n",
        "*   El ambiente genera una recompensa r\n",
        "*   El objetivo es maximizar la recompensa \n",
        "\n",
        "\n",
        "\n",
        "![Atari Game Agent and Environment](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcTg6W2PLH8geQ_yZDQ_rzQN1hRw3a_8rxJPT8EcyVnlD1a4E8JO)\n",
        "\n",
        "\n",
        "*Fuente:* [Microsoft Research](https://slivkins.com/work/bandits-svc/)\n",
        "\n",
        "\n",
        "TODO: \n",
        "- buscar un juego interactivo o animación en linea de MAB (o ver si es factible \n",
        "hacer uno sencillo con Python)\n",
        "- Usar notación y formatting de Latex\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HTmHkF9WfaB",
        "colab_type": "text"
      },
      "source": [
        "# Valor de las Acciones\n",
        "\n",
        "Función de las acciones.\n",
        "\n",
        "\n",
        "\n",
        "*   El valor de la acción \"a\" es la recompensa promedio para \"a\".\n",
        ">   $q(a) = E[R|A=a]$ \n",
        "*   Debemos aproximar o estimar $q(a)$ siendo la forma mas sencilla el promedio de las muestras de las recompensas.\n",
        ">   $Q_{t}(a) = \\frac{\\sum_{n=1}^{t}R_{n}I(A_{n}=a)}{\\sum_{n=1}^{t}I(A_{n}=a)}$\n",
        "\n",
        "  Donde $I(True) = 1$ y $I(False) = 0$\n",
        "\n",
        "Dicho de otra forma: **$Q(a)$** es una aproximación de **$q(a)$** obtenida a través de promediar las recompensas obtenidas al seleccionar la acción **$a$**\n",
        "\n",
        "TODO: terminar la diapositiva y arreglar formato y notación latex\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-U7niSYMNiBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  Nota: el siguiente código no busca ser óptimo ni usar buenas prácticas de \n",
        "  programación. Busca  ilustrar los conceptos y ser fácil de entender.\n",
        "  TODO: agregar mas comentarios y descripción\n",
        "\"\"\"\n",
        "\n",
        "class Agent:\n",
        "\n",
        "  def __init__(self,action_list = []):\n",
        "    self.action_list = action_list\n",
        "    self.Q = dict()\n",
        "    self.action_reward_history = dict()\n",
        "\n",
        "    for action in action_list:\n",
        "      self.action_reward_history[action] = []\n",
        "\n",
        "\n",
        "  def update_Q(self):\n",
        "    for action in self.action_list:\n",
        "      self.Q[action] = sum(self.action_reward_history[action])/len(self.action_reward_history[action])\n",
        "\n",
        "  def observe_action_reward(self,action,reward):\n",
        "    self.action_reward_history[action].append(reward)\n",
        "\n",
        "  def print_estimated_q(self):\n",
        "    for action in self.action_list:\n",
        "      print(\"--------\")\n",
        "      print(\"Estimado de acción {} es {}\".format(action,self.Q[action]))\n",
        "\n",
        "    print(\"--------\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrUJGiDbS_t-",
        "colab_type": "code",
        "outputId": "e10514e8-ef13-45e6-8216-a2f4a303b5d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "\"\"\"\n",
        "  Nota: en el siguiente ejemplo el agente no elige acciones y el entorno no provee las recompensas\n",
        "  - Nosotros elegimos manualmente las acciones.\n",
        "  - Nosotros damos recompensas arbitrarias.\n",
        "\"\"\"\n",
        "\n",
        "agent = Agent([1,2,3])\n",
        "\n",
        "# elegir una vez cada acción y observar la recompensa obtenida\n",
        "agent.observe_action_reward(1,2)\n",
        "agent.observe_action_reward(2,-1)\n",
        "agent.observe_action_reward(3,2)\n",
        "\n",
        "# actualizar el estimado de q(a) llamado Q(a) y mostrar el estimado\n",
        "agent.update_Q()\n",
        "agent.print_estimated_q()\n",
        "\n",
        "# elegir la segunda acción, observar la recompensa , actualizar el estimado de q(a) y mostrarlo\n",
        "agent.observe_action_reward(2,1)\n",
        "agent.update_Q()\n",
        "agent.print_estimated_q()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------\n",
            "Estimado de acción 1 es 2.0\n",
            "--------\n",
            "Estimado de acción 2 es -1.0\n",
            "--------\n",
            "Estimado de acción 3 es 2.0\n",
            "--------\n",
            "--------\n",
            "Estimado de acción 1 es 2.0\n",
            "--------\n",
            "Estimado de acción 2 es 0.0\n",
            "--------\n",
            "Estimado de acción 3 es 2.0\n",
            "--------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JixpNvuoY8N2",
        "colab_type": "text"
      },
      "source": [
        "## Valor de las Acciones (actualización incremental)\n",
        "\n",
        "En el caso anterior requerimos guardar la historia de recompensas \n",
        "\n",
        "*   Podemos obtener una expresión para una actualización incremental.\n",
        "  *   Requiere menos memoria\n",
        "  *   Menor tiempo de computo \n",
        "  > $Q_{t}(a_{t}) = Q_{t-1}(a_{t}) + α_{t}(R_{t} - Q_{t-1}(a_{t}))$\n",
        "\n",
        "\n",
        "\n",
        "*   Es posible(y lo haremos) utilizar otros tamaños de paso $α$(similar al learning rate de aprendizaje supervisado)\n",
        "  *   Por ejemplo un valor consante de $α$ llevaría a algo llamado \"tracking\" (contrario a promedio).\n",
        "      *   Similar a promedio ponderado dando mayor importancia a las muestras mas recientes\n",
        "\n",
        "TODO: terminar de agregar en latex la definición formal y agregar alguna referencia al promedio incremental.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HszUzmx7U_UW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "  Nota: el siguiente código no busca ser óptimo ni usar buenas prácticas de \n",
        "  programación. Busca  ilustrar los conceptos y ser fácil de entender.\n",
        "  TODO: agregar mas comentarios y descripción\n",
        "\"\"\"\n",
        "\n",
        "class Agent:\n",
        "\n",
        "  def __init__(self,action_list = []):\n",
        "    self.action_list = action_list\n",
        "    self.Q = {a:0 for a in action_list}\n",
        "    self.N = {a:0 for a in action_list}\n",
        "\n",
        "  def observe_action_reward(self,action,reward):\n",
        "    self.N[action] = self.N[action] + 1\n",
        "    alpha = 1/self.N[action]\n",
        "    self.Q[action] = self.Q[action] +  alpha*(reward - self.Q[action])\n",
        "    #TODO: podemos tener 2 versiones de esta celda, una que requiera a los participantes definir los calculos\n",
        "\n",
        "  def print_estimated_q(self):\n",
        "    for action in self.action_list:\n",
        "      print(\"--------\")\n",
        "      print(\"Estimado de acción {} es {}\".format(action,self.Q[action]))\n",
        "\n",
        "    print(\"--------\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjqcuNH0dGgJ",
        "colab_type": "code",
        "outputId": "020d8331-7d78-4043-92fe-bc53293dd35c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "\"\"\"\n",
        "  Nota: en el siguiente ejemplo el agente no elige acciones y el entorno no provee las recompensas\n",
        "  - Nosotros elegimos manualmente las acciones.\n",
        "  - Nosotros damos recompensas arbitrarias.\n",
        "\"\"\"\n",
        "\n",
        "agent = Agent([1,2,3])\n",
        "\n",
        "# elegir una vez cada acción y observar la recompensa obtenida\n",
        "agent.observe_action_reward(1,2)\n",
        "agent.observe_action_reward(2,-1)\n",
        "agent.observe_action_reward(3,2)\n",
        "\n",
        "# actualizar el estimado de q(a) llamado Q(a) y mostrar el estimado\n",
        "agent.print_estimated_q()\n",
        "\n",
        "# elegir la segunda acción, observar la recompensa , actualizar el estimado de q(a) y mostrarlo\n",
        "agent.observe_action_reward(2,1)\n",
        "agent.print_estimated_q()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------\n",
            "Estimado de acción 1 es 2.0\n",
            "--------\n",
            "Estimado de acción 2 es -1.0\n",
            "--------\n",
            "Estimado de acción 3 es 2.0\n",
            "--------\n",
            "--------\n",
            "Estimado de acción 1 es 2.0\n",
            "--------\n",
            "Estimado de acción 2 es 0.0\n",
            "--------\n",
            "Estimado de acción 3 es 2.0\n",
            "--------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "863dYF3mg3jf",
        "colab_type": "text"
      },
      "source": [
        "## Ejemplo de la Rata\n",
        "\n",
        "![rata_ejemplo3](https://github.com/llealgt/RL_Workshop_Guatemala/blob/master/Assets/lect2_rat3_sample.png?raw=true)\n",
        "\n",
        "\n",
        "*Fuente:* [**UCL**](https://hadovanhasselt.files.wordpress.com/2016/01/xx2.pdf)\n",
        "\n",
        "> $$\\begin{align}\n",
        "  \\ Q_{3}(palanca) = 0  \\\\\n",
        "  \\ Q_{3}(boton)   = -1 \\\\\n",
        "  \\end{align}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNyJh7bwcZJI",
        "colab_type": "code",
        "outputId": "b4098220-1efb-4265-ce50-b67b9ce1468c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "agent = Agent([\"palanca\",\"boton\"])\n",
        "rewards_dict = {\"queso\":1,\"shock\":-1}\n",
        "\n",
        "# elegir una vez cada acción y observar la recompensa obtenida\n",
        "agent.observe_action_reward(\"boton\",rewards_dict[\"shock\"]) #t1\n",
        "agent.observe_action_reward(\"palanca\",rewards_dict[\"queso\"]) #t2\n",
        "agent.observe_action_reward(\"palanca\",rewards_dict[\"shock\"]) #t3\n",
        "\n",
        "# actualizar el estimado de q(a) llamado Q(a) y mostrar el estimado\n",
        "agent.print_estimated_q()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------\n",
            "Estimado de acción palanca es 0.0\n",
            "--------\n",
            "Estimado de acción boton es -1.0\n",
            "--------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsw-7mxN69y",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "![rata_ejemplo3](https://github.com/llealgt/RL_Workshop_Guatemala/blob/master/Assets/lect2_rat4_sample.png?raw=true)\n",
        "\n",
        "\n",
        "*Fuente:* [**UCL**](https://hadovanhasselt.files.wordpress.com/2016/01/xx2.pdf)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Dada la historia entonces:\n",
        "\n",
        "> $\\begin{align}\n",
        "\\ Q_{6}(palanca)  =  -0.6\\\\\n",
        "\\ Q_{6}(boton)    = -1.0\\\\\n",
        "\\end{align}$\n",
        "\n",
        "*   Cuando debería dejar de ser codiciosa ?\n",
        "\n",
        "TODO: ver como alinear a la izquierda en latex\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qg2MrAn0ig_l",
        "colab_type": "code",
        "outputId": "be2400b1-f928-47f2-eabc-0628e66bfd76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "agent = Agent([\"palanca\",\"boton\"])\n",
        "rewards_dict = {\"queso\":1,\"shock\":-1}\n",
        "\n",
        "# elegir una vez cada acción y observar la recompensa obtenida\n",
        "agent.observe_action_reward(\"boton\",rewards_dict[\"shock\"]) #t1\n",
        "agent.observe_action_reward(\"palanca\",rewards_dict[\"queso\"]) #t2\n",
        "agent.observe_action_reward(\"palanca\",rewards_dict[\"shock\"]) #t3\n",
        "agent.observe_action_reward(\"palanca\",rewards_dict[\"shock\"]) #t4\n",
        "agent.observe_action_reward(\"palanca\",rewards_dict[\"shock\"]) #t5\n",
        "agent.observe_action_reward(\"palanca\",rewards_dict[\"shock\"]) #t6\n",
        "\n",
        "# actualizar el estimado de q(a) llamado Q(a) y mostrar el estimado\n",
        "agent.print_estimated_q()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------\n",
            "Estimado de acción palanca es -0.6\n",
            "--------\n",
            "Estimado de acción boton es -1.0\n",
            "--------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW7lzlO-lryZ",
        "colab_type": "text"
      },
      "source": [
        "## Valor Optimo y Acción Optima\n",
        "\n",
        "\n",
        "\n",
        "*   La acción óptima  $a^*$ es la acción que maximiza a la función de valor.\n",
        "  > $a^* =\\underset{a\\in A}{\\operatorname{argmax}}\\ q(a)$\n",
        "*   El valor óptimo $v_{*}$ es el valor de la acción óptima. \n",
        "  > $v_{*} = q(a^*) = \\underset{a\\in A}{\\operatorname{max}}\\ q(a) =\\underset{a\\in A}{\\operatorname{max}}\\ E[R|A=a]$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WNCeS-JklCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_optimos(q={}):\n",
        "  \"\"\"\n",
        "    Dado un diccionario q representando a la función de valor devuelve:\n",
        "      la acción optima\n",
        "      el valor óptimo\n",
        "  \"\"\"\n",
        "\n",
        "  accion_optima = list(q.keys())[0]\n",
        "  valor_optimo = q[accion_optima]\n",
        "\n",
        "  for accion in q.keys():\n",
        "    valor = q[accion]\n",
        "\n",
        "    if valor > valor_optimo:\n",
        "      valor_optimo = valor\n",
        "      accion_optima = accion\n",
        "\n",
        "  return accion_optima,valor_optimo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8QaWVI0uxs6",
        "colab_type": "text"
      },
      "source": [
        "Por ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TL5_cC58soWi",
        "colab_type": "code",
        "outputId": "7b266caf-a70d-417f-cc59-a578db0b9a51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "accion_optima,valor_optimo = calc_optimos({1:1,2:2,3:3,0:8,10:5})\n",
        "\n",
        "print(\"accion y valor optimos: {},{}\".format(accion_optima,valor_optimo))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accion y valor optimos: 0,8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uwx4LynZwWCa",
        "colab_type": "text"
      },
      "source": [
        "## Regret (arrepentimiento)\n",
        "\n",
        "El arrepentimiento es la oportunidad perdida en el momento t.\n",
        "\n",
        ">  $l_{t} = v_{*} - q(A_{t})$\n",
        "\n",
        "Ejemplo: si la acción optima es tomar el bus con un valor de 8 y en el momento\n",
        "\"t\" tomo un taxi con un valor de 5 mi arrepentimiento es de **3**.\n",
        "\n",
        "(perdí la oportunidad de tener 3 unidades mas)\n",
        "\n",
        "\n",
        "**Nota**\n",
        "*   El arrepentimiento es una herramienta para analizar diferentes algoritmos\n",
        "  *   No es parte de ningún algoritmo.\n",
        "  *   El agente no puede observarlo u obtener muestras del arrepentimiento real.\n",
        "  *   Depende de conocer el valor óptimo y esto es precisamente algo que el agente desconoce y busca aprender.\n",
        "  *   Herramienta para el desarrollador.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJTNqZzrs5RS",
        "colab_type": "code",
        "outputId": "e636b712-7f03-4a49-e3cf-325c2ec1ca37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "def calc_arrepentimiento(valor_optimo,q,accion_t):\n",
        "  l = valor_optimo - q[accion_t]\n",
        "\n",
        "  return l"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Arrepentimiento:\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nicMT-wzs8xx",
        "colab_type": "code",
        "outputId": "30ad8db9-c11b-4c71-e242-803e287d443d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "q = {1:1,2:2,3:3,0:8,10:5}\n",
        "accion_t = 1\n",
        "accion_optima,valor_optimo = calc_optimos(q)\n",
        "arrepentimiento = calc_arrepentimiento(valor_optimo,q,accion_t)\n",
        "\n",
        "print(\"Arrepentimiento de acción {} es {}\".format(accion_t,arrepentimiento))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Arrepentimiento de acción 1 es 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSKQlW_k0qDf",
        "colab_type": "text"
      },
      "source": [
        "**Arrepentimiento total**: es la oportunidad perdida total desde el tiempo i=1 hasta t\n",
        "\n",
        "> $L_{t} = \\sum_{i=1}^{t} l_{t} = \\sum_{i=1}^{t} (v_{*} - q(a_{i}))$\n",
        "\n",
        "\n",
        "\n",
        "*   **Objetivo:** Balancear la exploración y explotación de manera que se minimice  el arrepentimiento total. \n",
        "*   Maximizar recompensa acumulada ≡ minimizar arrepentimiento total.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1znCHQY1ke5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbAXyqWpKVUo",
        "colab_type": "text"
      },
      "source": [
        "**$A_{t}$**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Rl2evQCKfe0",
        "colab_type": "text"
      },
      "source": [
        "**$R_{t}$**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIu_EXWGKaOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}